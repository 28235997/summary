google file system(GFS)


## 架构
一个master,多个chunkserver(是chunkserver,不是chunk)
GFS存储的文件都被分割成固定大小的chunk,在chunk创建的时候,会被分配一个全球唯一标识,出于可靠性,会被复制到多个块服务器上.

master节点管理所有的文件系统元数据包括:
- 名字空间namespace
- 访问控制Access control
- 文件和chunk的映射信息
- 当前chunk位置信息
无论客户端还是chunk服务器都不需要缓存数据,因为文件太大,而且linux的文件系统会将经常访问的数据缓存到内存中

### chunk尺寸
也就是块的大小,为什么设计成64M?
1. 减少了客户端和master节点通讯的需求,因为一次就可以获取chunk的位置信息,之后可以多次交互
2. 采用较大的chunk尺寸,客户端能够对一个块进行多次操作,就能够保持较长时间的tcp连接来降低网络负载
3. 采用较大chunk可以减少master保存的元数据信息,可以将所有都保存在内存中.

有争议的就是有内部碎片很多.采用惰性空间分配也有其缺陷,

### 元数据
采用保存变更日志的方式，我们能够简单可靠的更新 Master 服务器的状态，并且不用担心 Master 服务器崩溃导致数据不一致的风险
master不会持久保存chunk的位置信息,有机器加入 ,master会轮询他们所存储的chunk位置信息

因为元数据在内存中,所以master的操作速度非常快,所以他会周期性的扫描chunk的位置信息

那么,Chunk 的数量以及整个系统的承载能力都受限于 Master服务器所拥有的内存大小。但是在实际应用中，这并不是一个严重的问题。
1. 一个chunk的信息不会大于64字节
2. 绝大多数chunk都是满的,也就是文件大部分大于64MB,只有最后一个是部分填充
3. 就算加内存费用也很少,

#### chunk位置信息
`Master 服务器并不保存持久化保存哪个 Chunk 服务器存有指定 Chunk 的副本的信息。 `
* 因为启动会轮询,之后的位置信息是被master控制的,而且周期性的心跳监控

#### 操作日志

