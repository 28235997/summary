google file system(GFS)


## 架构
一个master,多个chunkserver(是chunkserver,不是chunk)
GFS存储的文件都被分割成固定大小的chunk,在chunk创建的时候,会被分配一个全球唯一标识,出于可靠性,会被复制到多个块服务器上.

master节点管理所有的文件系统元数据包括:
- 名字空间namespace
- 访问控制Access control
- 文件和chunk的映射信息
- 当前chunk位置信息
- 租约时间，因为chunk有一个主chunk，顺序化
无论客户端还是chunk服务器都不需要缓存数据,因为文件太大,而且linux的文件系统会将经常访问的数据缓存到内存中

### chunk尺寸
也就是块的大小,为什么设计成64M?
1. 减少了客户端和master节点通讯的需求,因为一次就可以获取chunk的位置信息,之后可以多次交互
2. 采用较大的chunk尺寸,客户端能够对一个块进行多次操作,就能够保持较长时间的tcp连接来降低网络负载
3. 采用较大chunk可以减少master保存的元数据信息,可以将所有都保存在内存中.

有争议的就是有内部碎片很多.采用惰性空间分配也有其缺陷,

### 元数据
采用保存变更日志的方式，我们能够简单可靠的更新 Master 服务器的状态，并且不用担心 Master 服务器崩溃导致数据不一致的风险
master不会持久保存chunk的位置信息,有机器加入 ,master会轮询他们所存储的chunk位置信息

因为元数据在内存中,所以master的操作速度非常快,所以他会周期性的扫描chunk的位置信息

那么,Chunk 的数量以及整个系统的承载能力都受限于 Master服务器所拥有的内存大小。但是在实际应用中，这并不是一个严重的问题。
1. 一个chunk的信息不会大于64字节
2. 绝大多数chunk都是满的,也就是文件大部分大于64MB,只有最后一个是部分填充
3. 就算加内存费用也很少,

#### chunk位置信息
`Master 服务器并不保存持久化保存哪个 Chunk 服务器存有指定 Chunk 的副本的信息。 `
* 因为启动会轮询,之后的位置信息是被master控制的,而且周期性的心跳监控

#### 操作日志
操作日志包含了对元数据变更的历史记录，他是元数据唯一的持久化存储记录，
操作日志非常重要，必须确保日志文件的完整性，确保日志被完整的持久化之后才对客户端可见，否则就会丢失数据，所以日志会复制到多态服务器，
灾难恢复时，通过重演日志进行，为了缩短时间，需要保证日志足够小，所以每隔一段时间做一次checkpoint(快照)，
由于创建checkpiont需要一定的时间，为了保证checkpiont的过程中不会被阻塞，master使用独立线程进行，包含了之前的所有修改，


#### 一致性模型
GFS提供了宽松的一致性机制，

#### 一致性保障机制


## 系统交互

原则是尽可能减少所有操作和master的交互

### 租约(lease) 和变更顺序
变更可能改变元数据，变更操作会在chunk的所有副本上执行，使用租约机制，
master节点为一个chunk建立租约，叫主chunk，主chunk对所有chunk的修改操作进行序列化，其他chunk按照序列进行执行，目的是为了减少master的负担

### 数据流
为了提高网络效率，采用了数据流和控制流分开的措施，在控制流从客户机到主chunk再到所有二级副本的同时，数据流以`管道`方式进行传输，而不是分散推送
线性推送每台机器所有的出口带宽都是最快速度，而不用分配带宽

会在网络拓扑中选择一条没有接收过数据的，最近的，依次传输，网络拓扑很简答，所以可以通过ip计算距离


### 原子记录的追加
GFS 提供了原子的数据追加的方式，
如果记录追加操作在任何一个副本上失败了，客户端就需要重新进行操作


### 快照
快照可以瞬间完成对一个文件或者目录树做一个拷贝，copy-on-write技术

## master节点的操作

### 名称空间管理和锁


### 垃圾回收
不会立即删除，而是会将文件改成一个包含删除时间戳的隐藏的名字，master节点会对文件系统命名空间做常规扫描，会删除三天前的文件，可以设置


## 高可用性

chunk的复制，每个chunk都被复制到不同机架的不同机器上，用户可以为文件命名空间的不同部分设置不同的复制级别，

### master的复制
对master服务状态的修改成功的前提是，操作日志写入到master服务器的备节点和本机磁盘，
此外，GFS 中还有些“影子”Master 服务器，这些“影子”服务器在“主”Master 服务器宕机的时候提
供文件系统的只读访问。

### 数据完整性
每个 Chunk 服务器都使用 Checksum 来检查保存的数据是否损坏


GFS为什么要顺序化？
因为GFS的应用场景一般都是顺序读写大文件，根据google的业务需要，抓取的网页和日志文件都是以追加的方式写入的，基本不需要太多随机写入的能力



* 如果有一个1G的数据保存再GFS中，它会将数据放在各个chunk中，每个chunk为64M，当读取时，会询问master在哪个chunk中，进行读取。


* 当请求的数据跨越chunk时，比如请求两个字节的数据，分布在两个chunk，则master会将读请求分割为两次读请求，


* 写操作
第一种情况：no primary：
首先找到最新的chunk副本的chunk server集合，因为系统已经运行了很长时间，会因为一些故障导致有旧的副本。或者两天前更新的副本，而该服务器挂了两天，导致没有保存。
1. 他需要和chunkserver通信读取version最大的，从而找出最新的chunk副本，
2. 将这个chunk副本设置为primary，然后依次更新
3. 当要追加数据时，客户端将要追加的数据副本发送给primary，和所有的sesondaies，然后他们都将数据写入到一个临时位置，此时这些数据并不会立刻追加到文件中，  等parmary和secondary都收到yes后，客户端发送一条消息到primary，你和所有secondary都收到了我要追加的数据，然后会以某种顺序把数据追加。如果一些secondary发生了错误，那么他会向client报告错误，但是成功的server雀食追加了。
   `同学问题问失败时候此时读取会不会读取到不一致的？`
        答案是会的，取决于你读取到了哪一个server
        `接着问不是会检查版本号嘛？`
            答案是此时不会更新版本号，只有当master选取primary时才会更新版本号，此时都是最新的版本号。

当收不到primary的回复时，不能立即重新选primary，有可能只是网络原因， 当你重新选择，则会出现俩个primary，成为脑裂。


GFS无法保证强一致性，也没有采取某种机制将某些不能执行写操作的secondary剔除，而且追加可能会出现一些诡异的结果，

要设计高度一致性的系统：`可以采取两阶段提交，第一阶段先询问能不能执行此操作，能执行在执行，否则直接报告。`


# GFS的局限性
1. 单master，导致master负载过重
2. master不能自动切换，需要人工介入
3. 一致性问题
